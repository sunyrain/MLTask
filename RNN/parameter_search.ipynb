{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf1a24-2f8d-415e-8320-ec208230aae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 16:17:43,588] Trial 170 finished with value: 0.6489795918367347 and parameters: {'batch_size': 16, 'learning_rate': 0.00012435332289911552, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:18:33,070] Trial 171 finished with value: 0.5346938775510204 and parameters: {'batch_size': 16, 'learning_rate': 9.091082255253162e-05, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:19:22,150] Trial 172 finished with value: 0.2653061224489796 and parameters: {'batch_size': 16, 'learning_rate': 6.851977350074323e-05, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:20:34,333] Trial 173 finished with value: 0.49795918367346936 and parameters: {'batch_size': 16, 'learning_rate': 0.00010316681832017305, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:21:50,602] Trial 174 finished with value: 0.6040816326530613 and parameters: {'batch_size': 16, 'learning_rate': 0.00018298400129435703, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:23:05,237] Trial 175 finished with value: 0.673469387755102 and parameters: {'batch_size': 16, 'learning_rate': 8.285985776927369e-05, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:24:09,878] Trial 176 finished with value: 0.46530612244897956 and parameters: {'batch_size': 16, 'learning_rate': 4.21604623153724e-05, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 40}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:24:18,672] Trial 177 finished with value: 0.4204081632653061 and parameters: {'batch_size': 128, 'learning_rate': 0.00014205255938014045, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:24:25,478] Trial 178 finished with value: 0.5755102040816327 and parameters: {'batch_size': 128, 'learning_rate': 0.0001724091468922826, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:24:27,793] Trial 179 finished with value: 0.5061224489795918 and parameters: {'batch_size': 128, 'learning_rate': 0.00010954001368083962, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 10}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:24:35,028] Trial 180 finished with value: 0.5183673469387755 and parameters: {'batch_size': 16, 'learning_rate': 7.305214477127951e-05, 'hidden_size': 256, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:25:02,465] Trial 181 finished with value: 0.689795918367347 and parameters: {'batch_size': 16, 'learning_rate': 9.367455134782303e-05, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:25:29,947] Trial 182 finished with value: 0.6571428571428571 and parameters: {'batch_size': 16, 'learning_rate': 9.106902153558423e-05, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:25:57,705] Trial 183 finished with value: 0.6326530612244898 and parameters: {'batch_size': 16, 'learning_rate': 0.00012535301875615167, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:26:16,167] Trial 184 finished with value: 0.5387755102040817 and parameters: {'batch_size': 16, 'learning_rate': 0.0001590105884979098, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 20}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:26:22,933] Trial 185 finished with value: 0.5346938775510204 and parameters: {'batch_size': 128, 'learning_rate': 5.277615094670219e-05, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:26:50,613] Trial 186 finished with value: 0.8081632653061225 and parameters: {'batch_size': 16, 'learning_rate': 0.0001283474368950358, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "[I 2023-12-16 16:27:18,499] Trial 187 finished with value: 0.6489795918367347 and parameters: {'batch_size': 16, 'learning_rate': 0.0001266638975256503, 'hidden_size': 512, 'num_layers': 3, 'num_epochs': 30}. Best is trial 138 with value: 0.8081632653061225.\n",
      "/tmp/ipykernel_1622/2838088194.py:82: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "# 设定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# RNN 分类器\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 加载数据函数\n",
    "def load_data(file_name):\n",
    "    with np.load(file_name) as data:\n",
    "        data_array = data['data']\n",
    "        labels_array = data['labels']\n",
    "    return data_array, labels_array\n",
    "\n",
    "# 数据转换为 PyTorch 张量\n",
    "def to_tensor(data, labels):\n",
    "    data_tensor = torch.Tensor(data)\n",
    "    labels_tensor = torch.LongTensor(labels.argmax(axis=1))\n",
    "    return data_tensor, labels_tensor\n",
    "\n",
    "# 加载数据\n",
    "train_data, train_labels = load_data('trainset_normalized.npz')\n",
    "test_data, test_labels = load_data('testset_normalized.npz')\n",
    "\n",
    "train_data_tensor, train_labels_tensor = to_tensor(train_data, train_labels)\n",
    "test_data_tensor, test_labels_tensor = to_tensor(test_data, test_labels)\n",
    "\n",
    "# 模型参数\n",
    "input_size = 16  # 特征数量\n",
    "output_size = 5  # 输出类别数量\n",
    "\n",
    "# 训练和评估函数\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy\n",
    "\n",
    "# 定义目标函数\n",
    "def objective(trial):\n",
    "    # 超参数搜索空间\n",
    "    batch_size = int(trial.suggest_categorical('batch_size', [16, 32, 64, 128]))\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
    "    hidden_size = int(trial.suggest_categorical('hidden_size', [32, 64, 128, 256, 512, 1024]))\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 7)\n",
    "    num_epochs = int(trial.suggest_categorical('num_epochs', [10, 15,20, 25,30,40]))  # 固定为一个较小的数值以加快实验\n",
    "\n",
    "    # 数据加载\n",
    "    train_loader = DataLoader(TensorDataset(train_data_tensor, train_labels_tensor), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(test_data_tensor, test_labels_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 模型初始化\n",
    "    model = RNNClassifier(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练和评估\n",
    "    accuracy = train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, num_epochs)\n",
    "    return accuracy\n",
    "\n",
    "# 创建一个 Optuna study 对象并开始优化\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "# 打印最佳参数\n",
    "print(\"最佳参数: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe48f38-9916-4c99-9021-86026ce65a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
